{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and downsampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/xor_data.csv')\n",
    "\n",
    "y = data['y']\n",
    "X = data.drop(columns='y')\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=10000, random_state=42)\n",
    "for train_idx, _ in sss.split(X, y):\n",
    "    X_s = X.iloc[train_idx]\n",
    "    y_s = y.iloc[train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_folds(X, y, n_folds = 10):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    folds = []\n",
    "    splits = kf.split(X, y)\n",
    "    for train_index, val_index in splits:\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        folds.append({'Xt': X_train, 'Xv': X_val, 'yt': y_train, 'yv': y_val})\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = generate_folds(X_s, y_s, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_with_last(arr, target_length):\n",
    "    return arr + [arr[-1]] * (target_length - len(arr)) \n",
    "\n",
    "class FeedforwardNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act = nn.ReLU()  \n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "def plot_results(results_in): \n",
    "    plt.plot(results_in['train_losses_by_epochs'], label='Train Loss')\n",
    "    plt.plot(results_in['val_losses'], label='Validation Loss')\n",
    "    #plt.plot(results_in['train_losses_by_samples'])\n",
    "    plt.ylim(bottom=0) \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curve for Classification Problem 1')\n",
    "    plt.show()\n",
    "\n",
    "def save_json(data, path, indent=4):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=indent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd_model(fold, hidden_dim=2, input_dim = 2, output_dim = 2, batch_size=1, learning_rate = 0.1, max_epochs=10, to_print=True, patience=5): \n",
    "    X_val = torch.tensor(fold['Xv'].values, dtype=torch.float32)\n",
    "    y_val = torch.tensor(fold['yv'].values, dtype=torch.long)\n",
    "    X_train = torch.tensor(fold['Xt'].values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(fold['yt'].values, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = FeedforwardNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    per_sample_loss = []\n",
    "    times = []\n",
    "    samples_seen = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_improve = 0\n",
    "    global_start = time.time()\n",
    "    total_epochs_ran = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        local_start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_X, batch_y in loader: \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "            per_sample_loss.append(loss.item())\n",
    "            samples_seen += batch_y.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss / total\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)              \n",
    "            val_loss = criterion(val_outputs, y_val).item()  \n",
    "            _, val_pred = torch.max(val_outputs, 1)\n",
    "            val_correct = 0\n",
    "            val_total = y_val.size(0)\n",
    "            val_correct = (val_pred == y_val).sum().item()\n",
    "            val_acc = val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-6:  \n",
    "            best_val_loss = val_loss\n",
    "            epochs_since_improve = 0\n",
    "        else:\n",
    "            epochs_since_improve += 1\n",
    "\n",
    "        local_stop = time.time()\n",
    "        local_duration = local_stop - local_start\n",
    "        times.append(local_duration)\n",
    "        total_epochs_ran += 1\n",
    "\n",
    "        if to_print == True:\n",
    "            print(f\"Epoch {epoch+1}/{max_epochs} | \"\n",
    "                f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                f\"Train Acc: {train_acc:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | \"\n",
    "                f\"Val Acc: {val_acc:.4f} | \"\n",
    "                f\"Duration: {local_duration:.4f}\")\n",
    "            \n",
    "        if epochs_since_improve >= patience or all(abs(x - 1.0) < 1e-6 for x in val_accs[-3:]):\n",
    "            if to_print == True:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    global_end = time.time()\n",
    "    global_duration = global_end - global_start\n",
    "\n",
    "    results = {\n",
    "        \"model\": model,\n",
    "        \"train_losses_by_epochs\": train_losses,\n",
    "        \"train_losses_by_samples\": per_sample_loss,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"global_duration\": global_duration, \n",
    "        \"epoch_durations\": times,\n",
    "        \"total_epochs\": total_epochs_ran\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evals of dimension and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_hidden_layers(folds, dim_list = [1, 2, 3], max_epochs=20):\n",
    "    results = {}\n",
    "    for n in dim_list:\n",
    "        results_per_size = {}\n",
    "        for i in tqdm(range(len(folds))):\n",
    "            results_per_size[f'fold_{i}'] = train_sgd_model(folds[i], hidden_dim=n, max_epochs=max_epochs, to_print=False)\n",
    "        results[f'dim_{n}'] = results_per_size\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:40<06:02, 40.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:25<05:45, 43.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:02<04:43, 40.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:44<06:28, 64.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [04:27<04:43, 56.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [05:20<03:42, 55.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [06:32<03:02, 60.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [07:08<01:46, 53.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [08:00<00:52, 52.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [08:12<00:54, 54.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mfind_optimal_hidden_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mfind_optimal_hidden_layers\u001b[39m\u001b[34m(folds, max_dim, max_epochs)\u001b[39m\n\u001b[32m      8\u001b[39m     results_per_size = []\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m tqdm(folds):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         results_per_size.append(\u001b[43mtrain_sgd_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_print\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     11\u001b[39m     results.append(results_per_size)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_sgd_model\u001b[39m\u001b[34m(fold, hidden_dim, input_dim, output_dim, batch_size, learning_rate, max_epochs, to_print, patience)\u001b[39m\n\u001b[32m     33\u001b[39m loss = criterion(outputs, batch_y)\n\u001b[32m     34\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m train_loss += loss.item()\n\u001b[32m     37\u001b[39m _, predicted = torch.max(outputs, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Machine_Learning/ml_env/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28mself\u001b[39m = cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    372\u001b[39m profile_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Machine_Learning/ml_env/lib/python3.11/site-packages/torch/autograd/profiler.py:610\u001b[39m, in \u001b[36mrecord_function.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[38;5;28mself\u001b[39m.record = torch.ops.profiler._record_function_enter_new(\n\u001b[32m    606\u001b[39m         \u001b[38;5;28mself\u001b[39m.name, \u001b[38;5;28mself\u001b[39m.args\n\u001b[32m    607\u001b[39m     )\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_callbacks_on_exit:\n\u001b[32m    612\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dim_results = find_optimal_hidden_layers(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
